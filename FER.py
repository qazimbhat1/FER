# -*- coding: utf-8 -*-
"""Facial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V8UyIoFp0Dfa4zItiu6pJakex8xNmgzQ
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
# %cd /content/gdrive/My\ Drive/Projects/FER

import torchvision.transforms as transforms
import numpy as np
import h5py
import torch.utils.data as data
import torch
from torch.utils.data import DataLoader, Dataset
import torchvision
import torchvision.datasets as datasets
from PIL import Image
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import cv2
from google.colab.patches import cv2_imshow

class FER(data.Dataset):
  def __init__(self, split='Training', transform=None):
    self.transform = transform
    self.split = split
    self.data = h5py.File('./fer1.h5', 'r', driver='core')
    if self.split == 'Training':
      self.train_data = self.data['Training_pixel']
      self.train_labels = self.data['Training_label']
      self.train_data = np.asarray(self.train_data)
      self.train_data = self.train_data.reshape((28709, 48, 48))
    elif self.split == 'PublicTest':
      self.PublicTest_data = self.data['PublicTest_pixel']
      self.PublicTest_labels = self.data['PublicTest_label']
      self.PublicTest_data = np.asarray(self.PublicTest_data)
      self.PublicTest_data = self.PublicTest_data.reshape((3589, 48, 48))
    else:
      self.PrivateTest_data = self.data['PrivateTest_pixel']
      self.PrivateTest_label = self.data['PrivateTest_label']
      self.PrivateTest_data = np.asarray(self.PrivateTest_data)
      self.PrivateTest_data = self.PrivateTest_data.reshape((3589, 48, 48))
  def __len__(self):
    if self.split == 'Training':
      return len(self.train_data)
    elif self.split == 'PublicTest':
      return len(self.PublicTest_data)
    else:
      return len(self.PrivateTest_data)
  def __getitem__(self, index):
    if self.split == 'Training':
      img, target = self.train_data[index], self.train_labels[index]
    elif self.split == 'PublicTest':
      img, target = self.PublicTest_data[index], self.PublicTest_labels[index]
    else:
      img, target = self.PrivateTest_data[index], self.PrivateTest_label[index]
    img = img[:, :, np.newaxis]
    img = np.concatenate((img, img, img), axis=2)
    img = Image.fromarray(img)
    if self.transform is not None:
      img = self.transform(img)
    return img, target

transform = transforms.Compose(
    [transforms.RandomHorizontalFlip(),transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainset = FER(split='Training', transform= transform)

transform1 = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

testset = FER(split='PublicTest', transform= transform1)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True)
 type(trainloader)

testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)
type(testloader)

for img, target in trainloader:
  print(img.shape)
  print(target)
  break

img_data = img[0]
img_data.shape
np_image = img_data.numpy()                      
np_image = np.transpose(np_image, (1,2,0))
np_image = np_image*(0.5, 0.5, 0.5)+(0.5, 0.5, 0.5)       
print(np_image.shape)

plt.figure(figsize = (2,2))
plt.imshow(np_image)
plt.show()

_, axs = plt.subplots(1, 8, figsize=(12, 12))
axs = axs.flatten()
for img, ax in zip(img, axs):
  np_image = img.numpy()                      
  np_image = np.transpose(np_image, (1,2,0))
  np_image = np_image*(0.5, 0.5, 0.5)+(0.5, 0.5, 0.5)       
  ax.imshow(np_image)
plt.show()

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)
        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=2)
        self.conv2 = nn.Conv2d(16,64,3)
        self.conv3 = nn.Conv2d(64,128,3)
        self.conv4 = nn.Conv2d(128,256,3)
        self.fc1 = nn.Linear(256 * 1 * 1, 500)
        self.fc2 = nn.Linear(500, 200)
        self.fc3 = nn.Linear(200, 7)

    def forward(self, x):
      x = self.pool(F.relu(self.conv1(x)))
      x = self.pool(F.relu(self.conv2(x)))
      x = self.pool(F.relu(self.conv3(x)))
      x = self.pool(F.relu(self.conv4(x)))
      x = x.view(-1, 256 * 1 * 1)
      x = F.relu(self.fc1(x))
      x = F.relu(self.fc2(x))
      x = self.fc3(x)
      return(x)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
model = Net().to(device)
model

criteria = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_list = []
acc_list = []
val_list=[]
valoss_list = []

for e in range(100):
  loss_net= 0
  acc_net = 0 
  val_acc = 0
  val_loss = 0
  total = 0
  total1 = 0
  for images, labels in trainloader:
    images = images.to(device)
    labels = labels.to(device)
    pred = model(images)
    loss = criteria(pred,labels)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    _, pred = torch.max(pred.data,1)
    loss_net += loss.item()
    acc_net += pred.eq(labels.data).sum()
    total += labels.size(0)
  else:
    with torch.no_grad(): #we dont want to update gradient from validation set
      for images, labels in testloader:
        images = images.to(device)
        labels = labels.to(device)
        pred = model(images)
        loss_val = criteria(pred,labels)
        _, pred = torch.max(pred.data,1)
        val_loss += loss_val.item()
        val_acc += pred.eq(labels.data).sum()
        total1 += labels.size(0)
  val_loss /= total1
  valoss_list.append(val_loss)
  val_acc = (int(val_acc.data)/total1)*100.0
  val_list.append(val_acc)
  loss_net /= total
  acc_net = (int(acc_net.data)/total)*100.0
  loss_list.append(loss_net)
  acc_list.append(acc_net)
  
  print("Epoch:",e,"loss: ", loss_net, "acc:", acc_net, "Val Acc: ", val_acc, "Val_loss: ", val_loss)
  break
  torch.save(model.state_dict(), "/content/gdrive/My Drive/Projects/FER/model")

plt.plot(loss_list, label ='Training Loss')
plt.plot(valoss_list, label = 'Validation loss')
plt.legend()



plt.plot(acc_list, label ='Training Acc')
plt.plot(val_list, label = 'Validation Acc')
plt.legend()

model.load_state_dict(torch.load("/content/gdrive/My Drive/Projects/FER/model"))
model.eval()

emotion_dict = {0: "Angry", 1: "Disgust", 2: "Fear", 3: "Happy", 4: "Sad", 5: "Surprise", 6: "Neutral"}
cap = cv2.VideoCapture('/content/gdrive/My Drive/Projects/FER/Final.mp4')
im_vd = []
num = 0
while True:
    ret, frame = cap.read()
    if not ret:
      print("here")
      continue
        
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    faces = face_cascade.detectMultiScale(gray, 1.3, 5)

    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 1)
        roi_gray = gray[y:y + h, x:x + w]
        cropped_img = cv2.resize(roi_gray, (48, 48))
        cropped_img = np.expand_dims(cropped_img, -1)
        cropped_img = np.concatenate((cropped_img, cropped_img, cropped_img), axis=2)
        cropped_img = Image.fromarray(cropped_img)
        cropped_img = transform(cropped_img)
        cropped_img = cropped_img[np.newaxis, :, :]
        cropped_img = cropped_img.to(device)
        pred = model(cropped_img)
        pred = pred.cpu().data.numpy()
        pred1 = np.argmax(pred[0])
        pred_emo = emotion_dict[pred1]
        cv2.putText(frame, emotion_dict[int(np.argmax(pred[0]))], (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 1, cv2.LINE_AA)

  
    im_vd.append(frame)
    #cv2_imshow(resized_img)
    num = num+1
    if num == 150:#works on 100
        break
size=(1080,720)
out = cv2.VideoWriter('/content/gdrive/My Drive/Projects/FER/project8.mp4',cv2.VideoWriter_fourcc(*'MJPG'), 15, size)
for i in range(len(im_vd)):
    out.write(im_vd[i])
    #cv2_imshow(im_vd[i])

print('end')
out.release()
cv2.destroyAllWindows

cap.release()
cv2.destroyAllWindows()

























face_haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

cap=cv2.VideoCapture('gdrive/My Drive/Projects/FER/Final.mp4')
#cap = cv2.VideoCapture(0)  #for capturing a video stream using camera. Does not work on Colab
im_vd =[]
num = 0
while True:
    ret,test_img=cap.read()# captures frame and returns boolean value and captured image
    
    if not ret:
      print("here")
      continue
    gray_img= cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)

    faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)

    for (x,y,w,h) in faces_detected:
        cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=7)
        roi_gray=gray_img[y:y+w,x:x+h]#cropping region of interest i.e. face area from  image
        roi_gray=cv2.resize(roi_gray,(48,48))
        img_pixels = Image2.img_to_array(roi_gray)
        img_pixels = np.expand_dims(img_pixels, axis = 0)
        #img_pixels /= 255
        
        img_pixels = np.concatenate((img_pixels,img_pixels,img_pixels),axis =3) #concatenate along channel dimension
        img_pixels = img_pixels.astype(np.uint8)
        img_pixels = img_pixels.reshape((48,48,3))
        img_pixels = Image1.fromarray(img_pixels)

        img_pixels = transform(img_pixels)
        
        #img_pixels = img_pixels.transpose(2,0)
        
        img_pixels = img_pixels[np.newaxis,:,:,:]

        img_pixels = img_pixels.to(device)
        predictions = model(img_pixels)

        #to find max indexed array
        pred = predictions.cpu().data.numpy()
        max_index = np.argmax(pred[0])

        emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral' )
        predicted_emotion = emotions[max_index]
        
        cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)

    #resized_img = cv2.resize(test_img, (1000, 700))
    print(test_img.shape)
    im_vd.append(test_img)
    #cv2_imshow(resized_img)
    num = num+1
    if num == 100:#works on 100
        break
size=(1080,720)
out = cv2.VideoWriter('gdrive/My Drive/Projects/FER/project3.mp4',cv2.VideoWriter_fourcc(*'MJPG'), 15, size)
for i in range(len(im_vd)):
    out.write(im_vd[i])
    cv2_imshow(test_img)

out.release()
cv2.destroyAllWindows

